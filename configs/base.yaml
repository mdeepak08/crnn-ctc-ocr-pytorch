seed:
  seed: 42
  deterministic: true
  deterministic_warn_only: true

device:
  prefer: auto  # auto|cpu|cuda|mps

vocab:
  path: data/processed/vocab.json
  blank_idx: 0

data:
  img_h: 32
  img_w: 128
  max_len: 25
  lowercase: true
  strict_vocab: true

# Train-time augmentations (real-world robustness). Off by default; enable per experiment config.
augment:
  enabled: false
  perspective_p: 0.15
  perspective_distortion: 0.25
  affine_p: 0.35
  affine_degrees: 2.0
  affine_translate: 0.02
  affine_scale_min: 0.9
  affine_scale_max: 1.1
  photometric_p: 0.35
  brightness: 0.25
  contrast: 0.25
  blur_p: 0.15
  blur_radius_max: 1.2
  jpeg_p: 0.15
  jpeg_quality_min: 30
  jpeg_quality_max: 85

model:
  img_h: 32
  num_channels: 1
  cnn_out_channels: 256
  rnn_hidden: 256
  rnn_layers: 2
  rnn_type: lstm
  dropout: 0.1

train:
  epochs: 5
  batch_size: 64
  num_workers: 2
  lr: 0.0005
  weight_decay: 0.01
  grad_clip: 5.0
  amp: true
  log_dir: logs/tensorboard
  ckpt_dir: checkpoints
  early_stop_patience: 10
  # When resuming from a checkpoint, `best_metric` is used for:
  # - early stopping
  # - deciding when to overwrite `best_by_cer.pt`
  #
  # For fine-tuning on a *new* dataset, you typically want to reset this so
  # the new run can establish its own "best" metric on the new val set.
  reset_best_on_resume: false
  decoder_for_val: greedy  # greedy|beam
  beam_width: 5

eval:
  batch_size: 64
  num_workers: 2
  decoder: greedy  # greedy|beam
  beam_width: 5

